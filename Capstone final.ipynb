{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Project\n",
    "## Phase 1, Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading positive reviews from the folder and appending it to posrev from training\n",
    "import os\n",
    "import pandas as pd\n",
    "path = r'Y:\\Analytics Vidhya\\Data Science\\Assignment\\Dataset\\aclImdb\\train\\pos'\n",
    "\n",
    "posrev = []\n",
    "files = [path+'/'+f for f in os.listdir(path) if os.path.isfile(path+'/'+f)]\n",
    "for f in files:\n",
    "    with open (f, 'r',encoding='iso-8859-1') as myfile:\n",
    "        posrev.append(myfile.read())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bromwell High is a cartoon comedy. It ran at t...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Homelessness (or Houselessness as George Carli...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Brilliant over-acting by Lesley Ann Warren. Be...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This is easily the most underrated film inn th...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This is not the typical Mel Brooks film. It wa...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review     label\n",
       "0  Bromwell High is a cartoon comedy. It ran at t...  positive\n",
       "1  Homelessness (or Houselessness as George Carli...  positive\n",
       "2  Brilliant over-acting by Lesley Ann Warren. Be...  positive\n",
       "3  This is easily the most underrated film inn th...  positive\n",
       "4  This is not the typical Mel Brooks film. It wa...  positive"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Assigning the positive reviews to a dataframe\n",
    "trainpos=pd.DataFrame(posrev,columns=['review'])\n",
    "trainpos['label']='positive'\n",
    "trainpos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading negative reviews from the folder and appending it to negrev from training\n",
    "path = r'Y:\\Analytics Vidhya\\Data Science\\Assignment\\Dataset\\aclImdb\\train\\neg'\n",
    "negrev = []\n",
    "files1 = [path+'/'+f for f in os.listdir(path) if os.path.isfile(path+'/'+f)]\n",
    "for f in files1:\n",
    "    with open (f, 'r',encoding='iso-8859-1') as myfile:\n",
    "        negrev.append(myfile.read())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Story of a man who has unnatural feelings for ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Airport '77 starts as a brand new luxury 747 p...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This film lacked something I couldn't put my f...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sorry everyone,,, I know this is supposed to b...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>When I was little my parents took me along to ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review     label\n",
       "0  Story of a man who has unnatural feelings for ...  negative\n",
       "1  Airport '77 starts as a brand new luxury 747 p...  negative\n",
       "2  This film lacked something I couldn't put my f...  negative\n",
       "3  Sorry everyone,,, I know this is supposed to b...  negative\n",
       "4  When I was little my parents took me along to ...  negative"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Assigning the negative reviews to a dataframe\n",
    "trainneg=pd.DataFrame(negrev,columns=['review'])\n",
    "trainneg['label']='negative'\n",
    "trainneg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Concatinating both positive and negative reviews into a single dataframe named training which contains all the reviews from \n",
    "#the training folder\n",
    "training = pd.concat([trainpos, trainneg])\n",
    "training.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I went and saw this movie last night after bei...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Actor turned director Bill Paxton follows up h...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>As a recreational golfer with some knowledge o...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I saw this film in a sneak preview, and it is ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bill Paxton has taken the true story of the 19...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review     label\n",
       "0  I went and saw this movie last night after bei...  positive\n",
       "1  Actor turned director Bill Paxton follows up h...  positive\n",
       "2  As a recreational golfer with some knowledge o...  positive\n",
       "3  I saw this film in a sneak preview, and it is ...  positive\n",
       "4  Bill Paxton has taken the true story of the 19...  positive"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Reading positive reviews from the folder and appending it to posrev1 from testing\n",
    "#Assigning the positive reviews to a dataframe\n",
    "path = r'Y:\\Analytics Vidhya\\Data Science\\Assignment\\Dataset\\aclImdb\\test\\pos'\n",
    "posrev1 = []\n",
    "files2 = [path+'/'+f for f in os.listdir(path) if os.path.isfile(path+'/'+f)]\n",
    "for f in files2:\n",
    "    with open (f, 'r',encoding='iso-8859-1') as myfile:\n",
    "        posrev1.append(myfile.read())\n",
    "\n",
    "testpos=pd.DataFrame(posrev1,columns=['review'])\n",
    "testpos['label']='positive'\n",
    "testpos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Once again Mr. Costner has dragged out a movie...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This is an example of why the majority of acti...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>First of all I hate those moronic rappers, who...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Not even the Beatles could write songs everyon...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Brass pictures (movies is not a fitting word f...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review     label\n",
       "0  Once again Mr. Costner has dragged out a movie...  negative\n",
       "1  This is an example of why the majority of acti...  negative\n",
       "2  First of all I hate those moronic rappers, who...  negative\n",
       "3  Not even the Beatles could write songs everyon...  negative\n",
       "4  Brass pictures (movies is not a fitting word f...  negative"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Reading negative reviews from the folder and appending it to negrev1 from testing\n",
    "#Assigning the negative reviews to a dataframe\n",
    "path = r'Y:\\Analytics Vidhya\\Data Science\\Assignment\\Dataset\\aclImdb\\test\\neg'\n",
    "negrev1 = []\n",
    "files3 = [path+'/'+f for f in os.listdir(path) if os.path.isfile(path+'/'+f)]\n",
    "for f in files3:\n",
    "    with open (f, 'r',encoding='iso-8859-1') as myfile:\n",
    "        negrev1.append(myfile.read())\n",
    "\n",
    "testneg=pd.DataFrame(negrev1,columns=['review'])\n",
    "testneg['label']='negative'\n",
    "testneg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Concatinating both positive and negative reviews into a single dataframe named testing which contains all the reviews from \n",
    "#the testing folder\n",
    "testing = pd.concat([testpos, testneg])\n",
    "testing.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 1.2 - Cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing all the necessary packages\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer \n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing the data for easier manipulating\n",
    "training['review'] = training['review'].str.lower().str.replace('[^a-z]', ' ').str.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing stop words using a lambda function after downloading the stop words list from the nltk module\n",
    "training['review'] = training['review'].apply(lambda x: [word for word in x if word not in set(stopwords.words('english'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[bromwell, high, cartoon, comedy, ran, time, p...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[homelessness, houselessness, george, carlin, ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[brilliant, acting, lesley, ann, warren, best,...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[easily, underrated, film, inn, brooks, cannon...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[typical, mel, brooks, film, much, less, slaps...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review     label\n",
       "0  [bromwell, high, cartoon, comedy, ran, time, p...  positive\n",
       "1  [homelessness, houselessness, george, carlin, ...  positive\n",
       "2  [brilliant, acting, lesley, ann, warren, best,...  positive\n",
       "3  [easily, underrated, film, inn, brooks, cannon...  positive\n",
       "4  [typical, mel, brooks, film, much, less, slaps...  positive"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming the words snowball stemmer as it is slightly faster and accurate than porterstemmer\n",
    "#refer https://zhiyu-chen.gitbooks.io/notes-of-nlp-with-python-/content/stemmer.html\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "training['review'] = training['review'].apply(lambda x:[stemmer.stem(word) for word in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[bromwel, high, cartoon, comedi, ran, time, pr...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[homeless, houseless, georg, carlin, state, is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[brilliant, act, lesley, ann, warren, best, dr...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[easili, underr, film, inn, brook, cannon, sur...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[typic, mel, brook, film, much, less, slapstic...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review     label\n",
       "0  [bromwel, high, cartoon, comedi, ran, time, pr...  positive\n",
       "1  [homeless, houseless, georg, carlin, state, is...  positive\n",
       "2  [brilliant, act, lesley, ann, warren, best, dr...  positive\n",
       "3  [easili, underr, film, inn, brook, cannon, sur...  positive\n",
       "4  [typic, mel, brook, film, much, less, slapstic...  positive"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization using wordnetlemmatizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "training['review'] = training['review'].apply(lambda x:[wordnet_lemmatizer.lemmatize(word) for word in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[bromwel, high, cartoon, comedi, ran, time, pr...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[homeless, houseless, georg, carlin, state, is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[brilliant, act, lesley, ann, warren, best, dr...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[easili, underr, film, inn, brook, cannon, sur...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[typic, mel, brook, film, much, le, slapstick,...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review     label\n",
       "0  [bromwel, high, cartoon, comedi, ran, time, pr...  positive\n",
       "1  [homeless, houseless, georg, carlin, state, is...  positive\n",
       "2  [brilliant, act, lesley, ann, warren, best, dr...  positive\n",
       "3  [easili, underr, film, inn, brook, cannon, sur...  positive\n",
       "4  [typic, mel, brook, film, much, le, slapstick,...  positive"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining the words back and saving it as a csv for easier access and saving time\n",
    "training['review'] = training['review'].apply(lambda x: \" \".join([word for word in x]))\n",
    "training['review'].head()\n",
    "training.to_csv(\"training.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the packages again for the ease of reading it from the saved csv file\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = pd.read_csv(\"training.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count vectorizer for transforming the reviews into integer values for the counting the values\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cvp = CountVectorizer()\n",
    "cvn = CountVectorizer()\n",
    "trp = training[training[\"label\"] == \"positive\"]\n",
    "trn = training[training[\"label\"] == \"negative\"]\n",
    "pv = cvp.fit_transform(trp.review)\n",
    "nv = cvn.fit_transform(trn.review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('br', 49236), ('film', 25314), ('movi', 22666), ('one', 14173), ('like', 10463), ('time', 8499), ('good', 7844), ('see', 7493), ('stori', 7481), ('charact', 7076)]\n"
     ]
    }
   ],
   "source": [
    "#Most common words used in the positive reviews in the dataset\n",
    "sum_words = pv.sum(axis = 0)\n",
    "pos_freq = [(word, sum_words[0, idx]) for word, idx in cvp.vocabulary_.items()]\n",
    "pos_freq =sorted(pos_freq, key = lambda x: x[1], reverse=True)\n",
    "print(pos_freq[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('br', 52636), ('movi', 29051), ('film', 22896), ('one', 13576), ('like', 12343), ('make', 8243), ('bad', 8002), ('even', 7801), ('time', 7699), ('get', 7667)]\n"
     ]
    }
   ],
   "source": [
    "#Most common words used in the negative reviews in the dataset\n",
    "sum_words = nv.sum(axis = 0)\n",
    "neg_freq = [(word, sum_words[0, idx]) for word, idx in cvn.vocabulary_.items()]\n",
    "neg_freq =sorted(neg_freq, key = lambda x: x[1], reverse=True)\n",
    "print(neg_freq[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['br', 101872], ['movi', 51717], ['film', 48210], ['one', 27749], ['like', 22806], ['time', 16198], ['good', 15368], ['make', 15212], ['charact', 14184], ['get', 14143]]\n",
      "[['houseless', 1], ['godbi', 1], ['pyschosi', 1], ['listner', 1], ['stetner', 1], ['taupin', 1], ['sutdi', 1], ['alterior', 1], ['asesino', 1], ['milimet', 1]]\n"
     ]
    }
   ],
   "source": [
    "#Most common words used in the entire dataset \n",
    "cv = CountVectorizer()\n",
    "v = cv.fit_transform(training.review)\n",
    "cv.get_feature_names()\n",
    "sum_words = v.sum(axis = 0)\n",
    "word_freq = [[word, sum_words[0, idx]] for word, idx in cv.vocabulary_.items()]\n",
    "high_freq =sorted(word_freq, key = lambda x: x[1], reverse=True)\n",
    "low_freq =sorted(word_freq, key = lambda x: x[1], reverse=False)\n",
    "print(high_freq[:10])\n",
    "print(low_freq[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 3 -  Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x26c95d14cf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#WORDCLOUD\n",
    "#Pip installing the wordcloud module and plotting the wordcloud\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "text = training.review\n",
    "wordcloud = WordCloud(\n",
    "    width = 3000,\n",
    "    height = 2000,\n",
    "    background_color = 'black').generate(str(text))\n",
    "fig = plt.figure(\n",
    "    figsize = (40, 30),\n",
    "    facecolor = 'k',\n",
    "    edgecolor = 'k')\n",
    "plt.imshow(wordcloud, interpolation = 'bilinear')\n",
    "plt.axis('off')\n",
    "plt.tight_layout(pad=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aa', 'aaa', 'aaaaaaah', 'aaaaah', 'aaaaatch', 'aaaahhhhhhh', 'aaaand', 'aaaarrgh', 'aaah', 'aaargh']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0          aa\n",
       "1         aaa\n",
       "2    aaaaaaah\n",
       "3      aaaaah\n",
       "4    aaaaatch\n",
       "dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#HISTOGRAM\n",
    "#Manipulating the data into a series for ease of plotting the histogram for the frequency of words\n",
    "import matplotlib as plt\n",
    "import seaborn as sns\n",
    "wf = cv.get_feature_names()\n",
    "print(wf[:10])\n",
    "wf = pd.Series(wf)\n",
    "wf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x26c970a8828>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x26c97a69f28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "wf.value_counts().head(20).sort_index(ascending=False).plot.barh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning, removing stopwords, stemming and lemmatization done for the testing dataset\n",
    "testing['review'] = testing['review'].str.lower().str.replace('[^a-z]', ' ').str.split()\n",
    "testing['review'] = testing['review'].apply(lambda x: [word for word in x if word not in set(stopwords.words('english'))])\n",
    "testing['review'] = testing['review'].apply(lambda x:[stemmer.stem(word) for word in x])\n",
    "testing['review'] = testing['review'].apply(lambda x:[wordnet_lemmatizer.lemmatize(word) for word in x])\n",
    "testing['review'] = testing['review'].apply(lambda x: \" \".join([word for word in x]))\n",
    "testing.to_csv(\"testing.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatenating the training and testing dataset and saving it as a csv file for easy access\n",
    "total = pd.concat([training, testing])\n",
    "total.reset_index()\n",
    "total.to_csv(\"total.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "total = pd.read_csv(\"total.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 4 - Hypothesis testing\n",
    "#### In hypothesis tesing, The training dataset is used to evaluate the accuracy measures of each algorithm seperately and the one that has the highest accuracy is picked for the entire algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the training dataset into train and test dataset comfortable for classification Algorithms\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(training[\"review\"], training[\"label\"], test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy achieved is 0.85368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP-PC\\A3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1428: UserWarning: labels size, 2, does not match size of target_names, 12500\n",
      "  .format(len(labels), len(target_names))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "   positive       0.84      0.87      0.86      6252\n",
      "   negative       0.87      0.84      0.85      6248\n",
      "\n",
      "avg / total       0.85      0.85      0.85     12500\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[5447,  805],\n",
       "       [1024, 5224]], dtype=int64)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Naive Bayes Algorithm with the use of pipeline in which tfdif vectorizer and classifier itself is used\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nb_clf = Pipeline([('vect', TfidfVectorizer()), \n",
    "                      ('clf', MultinomialNB()) ])\n",
    "\n",
    "# train the model\n",
    "nb_clf.fit(X_train, y_train)\n",
    "# Predict the test cases\n",
    "predicted = nb_clf.predict(X_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "print('Accuracy achieved is ' + str(np.mean(predicted == y_test)))\n",
    "print(metrics.classification_report(y_test, predicted, target_names= y_test)),\n",
    "metrics.confusion_matrix(y_test, predicted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy achieved is 0.49984\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   positive       0.00      0.00      0.00      6252\n",
      "   negative       0.50      1.00      0.67      6248\n",
      "\n",
      "avg / total       0.25      0.50      0.33     12500\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP-PC\\A3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1428: UserWarning: labels size, 2, does not match size of target_names, 12500\n",
      "  .format(len(labels), len(target_names))\n",
      "C:\\Users\\HP-PC\\A3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[   0, 6252],\n",
       "       [   0, 6248]], dtype=int64)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Support Vector machines algorithm is applied with the same pipeline\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svc_clf = Pipeline([('vect', TfidfVectorizer()), \n",
    "                      ('clf', SVC()) ])\n",
    "\n",
    "# train the model\n",
    "svc_clf.fit(X_train, y_train)\n",
    "# Predict the test cases\n",
    "svc_predicted = svc_clf.predict(X_test)\n",
    "\n",
    "print('Accuracy achieved is ' + str(np.mean(svc_predicted == y_test)))\n",
    "print(metrics.classification_report(y_test, svc_predicted, target_names= y_test)),\n",
    "metrics.confusion_matrix(y_test, svc_predicted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy achieved is 0.73656\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   positive       0.74      0.74      0.74      6252\n",
      "   negative       0.74      0.74      0.74      6248\n",
      "\n",
      "avg / total       0.74      0.74      0.74     12500\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP-PC\\A3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1428: UserWarning: labels size, 2, does not match size of target_names, 12500\n",
      "  .format(len(labels), len(target_names))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[4614, 1638],\n",
       "       [1655, 4593]], dtype=int64)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random Forest Algorithm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_clf = Pipeline([('vect', TfidfVectorizer()), \n",
    "                      ('clf', RandomForestClassifier(n_estimators = 5)) ])\n",
    "\n",
    "# train the model\n",
    "rf_clf.fit(X_train, y_train)\n",
    "# Predict the test cases\n",
    "rf_predicted = rf_clf.predict(X_test)\n",
    "\n",
    "print('Accuracy achieved is ' + str(np.mean(rf_predicted == y_test)))\n",
    "print(metrics.classification_report(y_test, rf_predicted, target_names= y_test)),\n",
    "metrics.confusion_matrix(y_test, rf_predicted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy achieved is 0.72416\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   positive       0.72      0.72      0.72      6252\n",
      "   negative       0.72      0.73      0.72      6248\n",
      "\n",
      "avg / total       0.72      0.72      0.72     12500\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP-PC\\A3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1428: UserWarning: labels size, 2, does not match size of target_names, 12500\n",
      "  .format(len(labels), len(target_names))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[4518, 1734],\n",
       "       [1714, 4534]], dtype=int64)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Decision Tree Classifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf = DecisionTreeClassifier(criterion='gini')\n",
    "\n",
    "dt_clf = Pipeline([('vect', TfidfVectorizer()), \n",
    "                      ('clf', DecisionTreeClassifier(criterion = \"gini\")) ])\n",
    "\n",
    "# train the model\n",
    "dt_clf.fit(X_train, y_train)\n",
    "# Predict the test cases\n",
    "dt_predicted = dt_clf.predict(X_test)\n",
    "\n",
    "print('Accuracy achieved is ' + str(np.mean(dt_predicted == y_test)))\n",
    "print(metrics.classification_report(y_test, dt_predicted, target_names= y_test)),\n",
    "metrics.confusion_matrix(y_test, dt_predicted)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Out of all the algorithms, the Naive Bayes had the most accuracy with 0.853. It was also the quickest to compile.\n",
    "### SVM had the least accuracy with 0.499 and took the most time to compile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 5 - Supervised and Unsupervised Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive bayes for the total dataset \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(total[\"review\"], total[\"label\"], test_size=0.25, random_state=0)\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nb_clf = Pipeline([('vect', TfidfVectorizer()), \n",
    "                      ('clf', MultinomialNB()) ])\n",
    "\n",
    "# train the model\n",
    "nb_clf.fit(X_train, y_train)\n",
    "# Predict the test cases\n",
    "predicted = nb_clf.predict(X_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "print('Accuracy achieved is ' + str(np.mean(predicted == y_test)))\n",
    "print(metrics.classification_report(y_test, predicted, target_names= y_test)),\n",
    "metrics.confusion_matrix(y_test, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supervised Algorithm no. 2 -  RandomforestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_clf = Pipeline([('vect', TfidfVectorizer()), \n",
    "                      ('clf', RandomForestClassifier(n_estimators = 5)) ])\n",
    "\n",
    "# train the model\n",
    "rf_clf.fit(X_train, y_train)\n",
    "# Predict the test cases\n",
    "rf_predicted = rf_clf.predict(X_test)\n",
    "\n",
    "print('Accuracy achieved is ' + str(np.mean(rf_predicted == y_test)))\n",
    "print(metrics.classification_report(y_test, rf_predicted, target_names= y_test)),\n",
    "metrics.confusion_matrix(y_test, rf_predicted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I admit, the great majority of films released ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Take a low budget, inexperienced actors doubli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Everybody has seen 'Back To The Future,' right...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Doris Day was an icon of beauty in singing and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>After a series of silly, fun-loving movies, 19...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review\n",
       "0  I admit, the great majority of films released ...\n",
       "1  Take a low budget, inexperienced actors doubli...\n",
       "2  Everybody has seen 'Back To The Future,' right...\n",
       "3  Doris Day was an icon of beauty in singing and...\n",
       "4  After a series of silly, fun-loving movies, 19..."
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing the unsupervised data \n",
    "import os\n",
    "path = r'Y:\\Analytics Vidhya\\Data Science\\Assignment\\Dataset\\aclImdb\\train\\unsup'\n",
    "\n",
    "unsup = []\n",
    "files4 = [path+'/'+f for f in os.listdir(path) if os.path.isfile(path+'/'+f)]\n",
    "for f in files4:\n",
    "    with open (f, 'r',encoding='iso-8859-1') as myfile:\n",
    "        unsup.append(myfile.read())\n",
    "unlabelled =pd.DataFrame(unsup,columns=['review'])\n",
    "unlabelled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing, removing stop words, stemming and lemmatizing the unsupervised data and saving it in a csv\n",
    "unlabelled['review'] = unlabelled['review'].str.lower().str.replace('[^a-z]', ' ').str.split()\n",
    "unlabelled['review'] = unlabelled['review'].apply(lambda x: [word for word in x if word not in set(stopwords.words('english'))])\n",
    "unlabelled['review'] = unlabelled['review'].apply(lambda x:[stemmer.stem(word) for word in x])\n",
    "unlabelled['review'] = unlabelled['review'].apply(lambda x:[wordnet_lemmatizer.lemmatize(word) for word in x])\n",
    "unlabelled['review'] = unlabelled['review'].apply(lambda x: \" \".join([word for word in x]))\n",
    "unlabelled.to_csv(\"unlabelled.csv\")\n",
    "vectorizer = TfidfVectorizer()\n",
    "ul = vectorizer.fit_transform(unlabelled[\"review\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : br, movi, film, one, like, good, charact, watch, time, make\n",
      "1 : movi, film, br, one, like, watch, good, time, see, make\n"
     ]
    }
   ],
   "source": [
    "# Clustering using the Kmeans algorithm into 2 clusters, presumably positive and negative reviews\n",
    "from sklearn.cluster import KMeans\n",
    "words = vectorizer.get_feature_names()\n",
    "kmeans = KMeans(n_clusters = 2, init = 'k-means++', n_init = 5, n_jobs = 1)\n",
    "kmeans.fit(ul)\n",
    "common_words = kmeans.cluster_centers_.argsort()[:,-1:-11:-1]\n",
    "for num, centroid in enumerate(common_words):\n",
    "    print(str(num) + ' : ' + ', '.join(words[word] for word in centroid))\n",
    "# The most common words on cluster 0 are all attributed to positive review and cluster 1 negative clearly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {0 : \"positive\", 1 : \"negative\"}\n",
    "mapped = [mapping[x] for x in kmeans.labels_]\n",
    "unlabelled[\"labels\"] = mapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>admit great major film releas say dozen major ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>take low budget inexperienc actor doubl produc...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>everybodi seen back futur right whether like m...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dori day icon beauti sing act warm voic genius...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>seri silli fun love movi big year dori day yea...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review    labels\n",
       "0  admit great major film releas say dozen major ...  positive\n",
       "1  take low budget inexperienc actor doubl produc...  negative\n",
       "2  everybodi seen back futur right whether like m...  positive\n",
       "3  dori day icon beauti sing act warm voic genius...  negative\n",
       "4  seri silli fun love movi big year dori day yea...  positive"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabelled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : br, movi, film, one, like, good, watch, charact, time, get\n",
      "1 : br, movi, film, one, show, like, time, charact, stori, get\n",
      "2 : movi, br, watch, like, bad, one, good, realli, see, time\n",
      "3 : film, br, one, like, charact, movi, good, see, make, time\n"
     ]
    }
   ],
   "source": [
    "# Clustering using the Kmeans algorithm into 4 clusters, and checking if it can be labelled\n",
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters = 4, init = 'k-means++', n_jobs = 1)\n",
    "kmeans.fit(ul)\n",
    "common_words = kmeans.cluster_centers_.argsort()[:,-1:-11:-1]\n",
    "for num, centroid in enumerate(common_words):\n",
    "    print(str(num) + ' : ' + ', '.join(words[word] for word in centroid))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering using the Kmeans algorithm into 3 clusters, and checking if it can be labelled\n",
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters = 3, init = 'k-means++', n_jobs = 1)\n",
    "kmeans.fit(ul)\n",
    "common_words = kmeans.cluster_centers_.argsort()[:,-1:-11:-1]\n",
    "for num, centroid in enumerate(common_words):\n",
    "    print(str(num) + ' : ' + ', '.join(words[word] for word in centroid))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The dataset, when seperated into 4 clusters or 3 clusters, does not split the data which can be put into different classes.\n",
    "### Therefore the clustering cannot be labelled using any class."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
